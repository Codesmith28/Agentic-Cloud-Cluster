# File Transfer and Storage Implementation Summary

## Overview
This document summarizes the implementation of file transfer and storage capabilities in CloudAI's master-worker architecture. This feature allows users to retrieve files generated by containerized tasks on worker nodes.

## Implementation Date
November 17, 2025

## Branch
`sarthak/provision_file_transfer_and_storage`

---

## ğŸ¯ Feature Requirements

The implemented system provides:
1. **Task Naming**: Users can assign custom names to tasks
2. **Timestamp Tracking**: Automatic recording of task submission time
3. **Volume Mounting**: Container output directories mounted to host filesystem
4. **File Extraction**: Generated files collected from containers
5. **File Transfer**: Streaming file uploads from worker to master via gRPC
6. **User-based Organization**: Files stored and organized by user, task name, and timestamp
7. **Access Control**: Users can only access their own files
8. **Database Persistence**: File metadata stored in MongoDB for easy retrieval

---

## ğŸ“‹ Architecture Flow

```
User submits task with name
        â†“
Master assigns to worker (with timestamp)
        â†“
Worker runs container with volume mount: /var/cloudai/outputs/<task-id>/ â†’ /output
        â†“
Container writes files to /output
        â†“
Worker collects file list from mounted volume
        â†“
Worker streams files to master via UploadTaskFiles RPC
        â†“
Master stores files at: /var/cloudai/files/<user_id>/<task_name>/<timestamp>/<task_id>/
        â†“
Master stores metadata in MongoDB FILE_METADATA collection
        â†“
User can retrieve files via API (filtered by user_id)
```

---

## ğŸ”§ Changes Made

### 1. Proto Definitions (`proto/master_worker.proto`)

**Added Fields to Task Message:**
```protobuf
message Task {
  // ... existing fields
  string task_name = 10;      // User-defined task name
  int64 submitted_at = 11;    // Unix timestamp when task was submitted
}
```

**Enhanced TaskResult Message:**
```protobuf
message TaskResult {
  // ... existing fields
  string result_location = 5;    // Local path on worker where files are stored
  repeated string output_files = 6;  // List of output file paths
}
```

**Added File Transfer Messages:**
```protobuf
message FileChunk {
  string task_id = 1;
  string user_id = 2;
  string task_name = 3;
  string file_path = 4;
  bytes data = 5;
  bool is_last_chunk = 6;
  bool is_last_file = 7;
  int64 timestamp = 8;
}

message FileUploadAck {
  bool success = 1;
  string message = 2;
  int32 files_received = 3;
}
```

**Added RPC Method:**
```protobuf
service MasterWorker {
  rpc UploadTaskFiles(stream FileChunk) returns (FileUploadAck);
}
```

### 2. Worker Executor (`worker/internal/executor/executor.go`)

**Volume Mounting:**
- Modified `createContainer()` to mount `/var/cloudai/outputs/<task-id>` on host to `/output` in container
- Creates output directory automatically before container starts

**File Collection:**
- Added `TaskResult.ResultLocation` and `TaskResult.OutputFiles` fields
- Implemented `collectOutputFiles()` to list all files in output directory
- Implemented `extractOutputFiles()` as alternative using `CopyFromContainer()` (for non-volume approach)

**Key Code:**
```go
// Mount output directory
hostConfig := &container.HostConfig{
    Mounts: []mount.Mount{
        {
            Type:   mount.TypeBind,
            Source: fmt.Sprintf("/var/cloudai/outputs/%s", taskID),
            Target: "/output",
        },
    },
}
```

### 3. Worker Server (`worker/internal/server/worker_server.go`)

**File Upload Implementation:**
- Added `uploadOutputFiles()` method to stream files to master
- Integrated file upload into `executeTask()` workflow
- Splits files into 1MB chunks for streaming
- Reports upload success/failure

**Upload Process:**
1. Check if output files exist
2. Connect to master via gRPC
3. Stream each file in chunks
4. Mark last chunk and last file appropriately
5. Close stream and receive acknowledgment

### 4. Master Storage Service (`master/internal/storage/file_storage.go`)

**FileStorageService:**
- Manages file storage organization on disk
- Base directory: `/var/cloudai/files/`
- Directory structure: `<user_id>/<task_name>/<timestamp>/<task_id>/`

**Key Methods:**
- `ReceiveFileStream()`: Handles streaming file uploads from workers
- `GetTaskStoragePath()`: Generates organized directory paths
- `ListUserFiles()`: Lists all files for a specific user
- `GetTaskFiles()`: Retrieves files for a specific task
- `DeleteTaskFiles()`: Removes task files
- `GetFilePath()`: Gets absolute path to a specific file

### 5. Master Database (`master/internal/db/file_metadata.go`)

**FILE_METADATA Collection:**
```go
type FileMetadata struct {
    UserID      string    `bson:"user_id"`
    TaskID      string    `bson:"task_id"`
    TaskName    string    `bson:"task_name"`
    Timestamp   time.Time `bson:"timestamp"`
    FilePaths   []string  `bson:"file_paths"`
    StoragePath string    `bson:"storage_path"`
    UploadedAt  time.Time `bson:"uploaded_at"`
}
```

**Indexes:**
- Unique index on `task_id`
- Index on `user_id`
- Index on `task_name`
- Compound index on `(user_id, timestamp)` for efficient user queries

**Key Methods:**
- `CreateFileMetadata()`: Store file metadata
- `GetFileMetadataByTask()`: Retrieve by task ID
- `GetFileMetadataByUser()`: Get all files for a user
- `GetFileMetadataByUserAndTaskName()`: Filter by user and task name
- `DeleteFileMetadata()`: Remove metadata

### 6. Master Server (`master/internal/server/master_server.go`)

**Added Fields:**
```go
type MasterServer struct {
    // ... existing fields
    fileMetadataDB   *db.FileMetadataDB
    fileStorage      *storage.FileStorageService
}
```

**UploadTaskFiles RPC Handler:**
- Receives file stream from worker
- Stores files in organized directory structure
- Stores metadata in database
- Returns upload acknowledgment

**Updated Constructor:**
```go
func NewMasterServer(
    workerDB, taskDB, assignmentDB, resultDB *db.XXX,
    fileMetadataDB *db.FileMetadataDB,
    fileStorage *storage.FileStorageService,
    telemetryMgr *telemetry.TelemetryManager,
) *MasterServer
```

### 7. Master Main (`master/main.go`)

**Initialization:**
- Initialize `FileMetadataDB` with MongoDB connection
- Initialize `FileStorageService` with base directory `/var/cloudai/files`
- Pass both to `NewMasterServer()`
- Graceful error handling if services fail to initialize

---

## ğŸ“Š Database Schema

### New Collection: FILE_METADATA

```javascript
{
  "_id": ObjectId,
  "user_id": "user-123",
  "task_id": "task-1731677400",
  "task_name": "ml-training-job",
  "timestamp": ISODate("2025-11-17T10:30:00Z"),
  "file_paths": [
    "result.json",
    "model.pkl",
    "metrics.csv"
  ],
  "storage_path": "/var/cloudai/files/user-123/ml-training-job/2025-11-17_10-30-00/task-1731677400",
  "uploaded_at": ISODate("2025-11-17T10:35:00Z")
}
```

**Indexes:**
- `task_id` (unique)
- `user_id`
- `task_name`
- `(user_id, timestamp)` compound

---

## ğŸ” Security & Access Control

### User-based Isolation

1. **File Storage**: Files organized by `user_id` at root level
2. **Database Queries**: Filtered by `user_id` to prevent cross-user access
3. **API Endpoints** (to be implemented): Will validate user authentication before file access

### Future Enhancements

- JWT-based authentication for file downloads
- Presigned URLs for direct file access
- File encryption at rest
- Quota management per user

---

## ğŸ§ª Testing Steps

### 1. Build the System
```bash
cd /home/codesmith28/Projects/CloudAI
make proto  # Regenerate proto code
make master # Build master node
make worker # Build worker node
```

### 2. Start Services
```bash
# Terminal 1: Start MongoDB
cd database && docker-compose up -d

# Terminal 2: Start Master
cd master && ./masterNode

# Terminal 3: Start Worker
cd worker && ./workerNode
```

### 3. Register Worker
```bash
# In master CLI
register <worker-id> <worker-ip>
```

### 4. Submit Task with File Output
```bash
# Use the sample task that writes to /output/result.json
task <task-name> <docker-image> -cpu_cores 1.0 -mem 0.5
```

### 5. Verify File Storage
```bash
# Check worker local storage
ls -la /var/cloudai/outputs/<task-id>/

# Check master storage
ls -la /var/cloudai/files/<user-id>/<task-name>/

# Check database
mongo cluster_db
> db.FILE_METADATA.find({user_id: "admin"}).pretty()
```

---

## ğŸ“‚ File Organization Example

```
/var/cloudai/files/
â””â”€â”€ admin/                          # user_id
    â””â”€â”€ ml-training/                # task_name
        â””â”€â”€ 2025-11-17_10-30-00/    # timestamp
            â””â”€â”€ task-1731677400/    # task_id
                â”œâ”€â”€ result.json
                â”œâ”€â”€ model.pkl
                â””â”€â”€ metrics/
                    â”œâ”€â”€ accuracy.csv
                    â””â”€â”€ loss.csv
```

---

## ğŸš€ Next Steps (TODO)

### 8. Update CLI for Task Naming
- [ ] Modify `master/internal/cli/cli.go` to accept task name parameter
- [ ] Update `task` command syntax: `task <task-name> <docker-image> [options]`
- [ ] Add timestamp generation on task submission
- [ ] Update TaskDB schema to include `task_name` and `submitted_at` fields

### 9. Create File Retrieval API
- [ ] Add HTTP endpoints in `master/internal/http/`
  - `GET /api/files` - List user's files
  - `GET /api/files/:task_id` - Get files for specific task
  - `GET /api/files/:task_id/:file_path` - Download specific file
  - `DELETE /api/files/:task_id` - Delete task files
- [ ] Implement user authentication/authorization
- [ ] Add file streaming for large files
- [ ] Add metadata filtering (by task name, date range)

### 10. Sample Task Update
- [ ] Update `SAMPLE_TASKS/task1/task.py` to demonstrate file output
- [ ] Add example with multiple file types (CSV, JSON, images)
- [ ] Document expected output structure

---

## ğŸ’¡ Usage Examples

### For Users

```bash
# Submit a task with name
master> task ml-training-v1 myuser/ml-image:latest -cpu_cores 2.0 -mem 4.0

# List your files (via API - to be implemented)
curl http://master:8080/api/files

# Download a specific file
curl http://master:8080/api/files/task-123/result.json -o result.json
```

### For Developers

```python
# In your containerized task (task.py)
import json

# Write output files to /output directory
results = {"accuracy": 0.95, "loss": 0.05}
with open("/output/results.json", "w") as f:
    json.dump(results, f)

# Files will automatically be:
# 1. Collected by worker
# 2. Uploaded to master
# 3. Organized by user/task/timestamp
# 4. Stored in database
```

---

## ğŸ‰ Benefits

1. **No Manual File Management**: Automatic collection and transfer
2. **Organized Storage**: Easy to find files by user, task, and date
3. **Scalable**: Streaming upload handles large files
4. **Persistent**: Files and metadata survive system restarts
5. **Secure**: User-based access control prevents data leaks
6. **Queryable**: MongoDB indexes enable fast file lookups

---

## ğŸ“ Notes

- File storage uses local filesystem (can be extended to S3/MinIO)
- Worker cleanup now preserves files after container removal
- Master stores both files and metadata for redundancy
- System supports any file type (binary or text)
- Chunk size is 1MB for streaming (configurable)

---

## ğŸ› Known Limitations

1. CLI not yet updated for task naming (still needed)
2. HTTP API for file retrieval not implemented (to be added)
3. No file size limits or quotas (should be added)
4. No file compression (could optimize storage)
5. No automatic cleanup of old files (retention policy needed)

---

## ğŸ”— Related Files

### Modified Files:
- `proto/master_worker.proto`
- `worker/internal/executor/executor.go`
- `worker/internal/server/worker_server.go`
- `master/internal/server/master_server.go`
- `master/main.go`

### New Files:
- `master/internal/storage/file_storage.go`
- `master/internal/db/file_metadata.go`

### Generated Files (via make proto):
- `proto/pb/*.pb.go`
- `proto/py/*_pb2.py`

---

## âœ… Build Status

- âœ… Proto generation successful
- âœ… Worker builds without errors
- âœ… Master builds without errors
- â³ Integration testing pending
- â³ CLI updates pending
- â³ HTTP API pending

---

**Implementation by**: GitHub Copilot  
**Date**: November 17, 2025  
**Status**: Core implementation complete, API endpoints pending
